{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 2\n",
        "\n",
        "Student Name: **Jash Prakash Rana**\n",
        "\n",
        "Student ID: **22222806**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk, re, string, warnings, random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from typing import List\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(2023)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSCr4xKJnhZ6"
      },
      "source": [
        "# Overview\n",
        "**Assignment 2** focuses on the training on a Neural Machine Translation (NMT) system for English-Irish translation where English is the source language and Irish is the target language. \n",
        "\n",
        "**Grading Policy** \n",
        "Assignment 2 is graded and will be worth 25% of your overall grade. This assignment is worth a total of 50 points distributed over the tasks below.  Please note that this is an individual assignment and you must not work with other students to complete this assessment. Any copying from other students, from student exercises from previous years, and any internet resources will not be tolerated. Plagiarised assignments will receive zero marks and the students who commit this act will be reported. Feel free to reach out to the TAs and instructors if you have any questions.\n",
        "\n",
        "## Task 1 - Data Collection and Preprocessing (10 points)\n",
        "## Task 1a. Data Loading (5 pts)\n",
        "Dataset: https://www.dropbox.com/s/zkgclwc9hrx7y93/DGT-en-ga.txt.zip?dl=0 \n",
        "*  Download a English-Irish dataset and decompress it. The `DGT.en-ga.en` file contains a list english sentences and `DGT.en-ga.ga` contains the paralell Irish sentences. Read both files into the Jupyter environment and load them into a pandas dataframe. \n",
        "* Randomly sample 12,000 rows.\n",
        "* Split the sampled data into train (10k), development (1k) and test set (1k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "mjieQgrsocnh"
      },
      "outputs": [],
      "source": [
        "# Your Code Here\n",
        "'''\n",
        "To load the model faster, I have \n",
        "taken 12000 random samples from both\n",
        "the dataset and made a new csv file\n",
        "to load the data faster.\n",
        "'''\n",
        "# df1 = pd.read_csv(\"./data/DGT.en-ga.en\", sep = '.', header = None, error_bad_lines = False)\n",
        "# df2 = pd.read_csv(\"./data/DGT.en-ga.ga\", sep = '.', header = None, error_bad_lines = False)\n",
        "\n",
        "# df1.to_csv('./data/dataframe1.csv')\n",
        "# df2.to_csv('./data/dataframe2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('./data/dataframe1.csv')\n",
        "df2 = pd.read_csv('./data/dataframe2.csv')\n",
        "\n",
        "df1 = df1.drop('Unnamed: 0', axis = 1)\n",
        "df2 = df2.drop('Unnamed: 0', axis = 1)\n",
        "\n",
        "df1 = df1.rename(columns = {\"0\": \"EngText\"})\n",
        "df2 = df2.rename(columns = {\"0\": \"IrText\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1 = df1[df1.index.isin(df2.index)]\n",
        "\n",
        "df1_sp = df1.sample(n = 12000, random_state = 2023)\n",
        "df2_sp = df2[df2.index.isin(df1_sp.index)]\n",
        "\n",
        "df1_sp = df1_sp.sort_index()\n",
        "df2_sp = df2_sp.sort_index()\n",
        "\n",
        "assert df1_sp.shape[0] == df2_sp.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "df1_train = df1_sp.sample(n = 10000, random_state = 2023)\n",
        "df1_sp = df1_sp[df1_sp.index.isin(df1_train.index) == False]\n",
        "df1_dev = df1_sp.sample(n = 1000, random_state = 2023)\n",
        "df1_sp = df1_sp[df1_sp.index.isin(df1_dev.index) == False]\n",
        "df1_test = df1_sp\n",
        "\n",
        "df1_train = df1_train.sort_index()\n",
        "df1_dev = df1_dev.sort_index()\n",
        "df1_test = df1_test.sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [],
      "source": [
        "df2_train = df2_sp[df2_sp.index.isin(df1_train.index)]\n",
        "df2_dev = df2_sp[df2_sp.index.isin(df1_dev.index)]\n",
        "df2_test = df2_sp[df2_sp.index.isin(df1_test.index)]\n",
        "\n",
        "df2_train = df2_train.sort_index()\n",
        "df2_dev = df2_dev.sort_index()\n",
        "df2_test = df2_test.sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert df1_train.shape[0] == df2_train.shape[0]\n",
        "assert df1_dev.shape[0] == df2_dev.shape[0]\n",
        "assert df1_test.shape[0] == df2_test.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.concat([df1_train, df2_train], axis=1).reset_index()\n",
        "df_test = pd.concat([df1_test, df2_test], axis=1).reset_index()\n",
        "df_dev = pd.concat([df1_dev, df2_dev], axis=1).reset_index()\n",
        "\n",
        "df_train = df_train.drop(\"index\", axis = 1)\n",
        "df_test = df_test.drop(\"index\", axis = 1)\n",
        "df_dev = df_dev.drop(\"index\", axis = 1)\n",
        "df_test1 = df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EngText</th>\n",
              "      <th>IrText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>in Scotland, the Court of Session, or in the c...</td>\n",
              "      <td>in Albain, an Court of Session nó, i gcás brei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HAVE in this spirit DECIDED to conclude this C...</td>\n",
              "      <td>TAR ÉIS COMHAONTÚ MAR A LEANAS:</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TITLE I</td>\n",
              "      <td>RAON FEIDHME</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TITLE II</td>\n",
              "      <td>DLÍNSE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SECTION 1</td>\n",
              "      <td>Forálacha Ginearálta</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             EngText  \\\n",
              "0  in Scotland, the Court of Session, or in the c...   \n",
              "1  HAVE in this spirit DECIDED to conclude this C...   \n",
              "2                                            TITLE I   \n",
              "3                                           TITLE II   \n",
              "4                                          SECTION 1   \n",
              "\n",
              "                                              IrText  \n",
              "0  in Albain, an Court of Session nó, i gcás brei...  \n",
              "1                    TAR ÉIS COMHAONTÚ MAR A LEANAS:  \n",
              "2                                       RAON FEIDHME  \n",
              "3                                             DLÍNSE  \n",
              "4                               Forálacha Ginearálta  "
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ejav7LUqokNc"
      },
      "source": [
        "## Task 1b. Preprocessing (5 pts)\n",
        "* Add '<bof\\>' to denote beginning of sentence and '<eos\\>' to denote the end of the sentence to each target line.\n",
        "* Perform the following pre-processing steps:\n",
        "  * Lowercase the text\n",
        "  * Remove all punctuation\n",
        "  * tokenize the text \n",
        "*  Build seperate vocabularies for each language. \n",
        "  * Assign each unique word an id value \n",
        "*Print statistics on the selected dataset:\n",
        "  * Number of samples\n",
        "  * Number of unique source language tokens\n",
        "  * Number of unique target language tokens\n",
        "  * Max sequence length of source language\n",
        "  * Max sequence length of target language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ref: Dr Paul Buitelaar/Dr Omnia Zayed - Lab 08 \"Neural NMT\"\n",
        "\n",
        "'''\n",
        "The Language class 'Lang' is used for\n",
        "preprocessing, encoding sentences,\n",
        "decoding IDs and storing words as a\n",
        "unique ID and counting no. of words for\n",
        "each language\n",
        "'''\n",
        "class Lang:\n",
        "    def __init__(self, language: str):\n",
        "        self.language = language\n",
        "        self.word2index = {\"PAD\": 0, \"<bof>\": 1, \"<eof>\": 2}\n",
        "        self.index2word = {0: \"PAD\", 1: \"<bof>\", 2: \"<eof>\"}\n",
        "        self.word2count = {\"<bof>\": 0, \"<eof>\": 0}\n",
        "        self.n_words = len(self.index2word)\n",
        "        self.longestSent = \"\"\n",
        "        self.longestIndex = 0\n",
        "    \n",
        "    def longest_sentence(self, sent: str,index):\n",
        "        if len(sent) > len(self.longestSent):\n",
        "            self.longestSent = sent\n",
        "            self.index = index\n",
        "\n",
        "    def preprocessing(self, data,index):\n",
        "        data = data.lower()\n",
        "        data = re.sub(r'[^\\w\\s]', '', data).strip()\n",
        "        # data = data.translate(string.punctuation)\n",
        "        data = data.replace(\"\\n\", \" \")\n",
        "        self.longest_sentence(data,index)\n",
        "        data = word_tokenize(data)\n",
        "        data = [\"<bof>\"] + data + [\"<eof>\"]\n",
        "        for word in data:\n",
        "            if word not in self.word2index:\n",
        "                self.word2index[word] = self.n_words\n",
        "                self.word2count[word] = 1\n",
        "                self.index2word[self.n_words] = word\n",
        "                self.n_words += 1\n",
        "            else:\n",
        "                self.word2count[word] += 1\n",
        "    \n",
        "    def encodeSent(self, sent: str) -> List[int]:\n",
        "        sent = sent.lower()\n",
        "        sent = re.sub(r'[^\\w\\s]', '', sent).strip()\n",
        "        sent = sent.replace(\"\\n\", \" \")\n",
        "        sent = word_tokenize(sent)\n",
        "        sent = [\"<bof>\"] + sent + [\"<eof>\"]\n",
        "        return [self.word2index[word] for word in sent if word in self.word2index]\n",
        "\n",
        "\n",
        "    def decodeIds(self, ids: list) -> List[str]:\n",
        "        return \" \".join([self.index2word[tok] for tok in ids])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80d2be10b85b4558a1c43461b71a7e1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "english = Lang(\"english\")\n",
        "irish = Lang(\"irish\")\n",
        "\n",
        "for index, row in tqdm(df_train.iterrows(), total=len(df_train)):\n",
        "  english.preprocessing(row[\"EngText\"],index)\n",
        "  irish.preprocessing(row[\"IrText\"],index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples per language: 10000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of samples per language: {len(df_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of English vocab: 9039\n",
            "Size of Irish vocab: 12173\n"
          ]
        }
      ],
      "source": [
        "print(f\"Size of English vocab: {english.n_words}\")\n",
        "print(f\"Size of Irish vocab: {irish.n_words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longest sentence in English: 777\n",
            "Longest sentence in Irish: 1415\n"
          ]
        }
      ],
      "source": [
        "print(f\"Longest sentence in English: {len(english.longestSent)}\")\n",
        "print(f\"Longest sentence in Irish: {len(irish.longestSent)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "article 1 this regulation lays down rules concerning the applicability of articles 101 to 106 and of article 1081 and 3 of the treaty on the functioning of the european union tfeu in relation to production of or trade in the products listed in annex i to the tfeu with the exception of the products covered by council regulation ec no 12342007 and regulation eu no 13792013 of the european parliament and of the councilarticle 45 amendments to regulation ec no 12242009 regulation ec no 12242009 is hereby amended as follows in article 571 the following sentences are added article 585 is amended as follows point g is replaced by the following g the information to consumers provided for in article 35 of regulation eu no 13792013 of the european parliament and of the council\n"
          ]
        }
      ],
      "source": [
        "print(english.longestSent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "airteagal 24b déanfaidh an coimisiún faisnéis a áireamh maidir le cur chun feidhme an rialacháin seo ina thuarascáil bhliantúil maidir le bearta cosanta trádála a chur i bhfeidhm agus a chur chun feidhme chuig parlaimint na heorpa agus chuig an gcomhairle de bhun airteagal 22a de rialachán ce uimh 12252009 ón gcomhairle15 rialachán ce uimh 1402008 maidir le rialachán ce uimh 1402008 is gá coinníollacha aonfhoirmeacha chun bearta cosanta agus bearta eile a ghlacadh i ndáil le clásail dhéthaobhacha chosanta an chomhaontaithe eatramhaigh agus an chomhaontaithe um chobhsaíocht agus um chomhlachas a chur chun feidhme ba cheart don choimisiún gníomhartha cur chun feidhme atá infheidhme láithreach a ghlacadh más rud é i gcásanna a bhfuil údar cuí leo a bhaineann le himthosca eisceachtúla agus géibheannacha a thagann chun cinn de réir bhrí airteagal 265b agus airteagal 274 den chomhaontú eatramhach agus dá éis airteagal 415b agus airteagal 424 den chomhaontú cobhsaíochta agus comhlachais go néilítear é sin ar mhórchúiseanna práinne dá réir sin leasaítear rialachán ce uimh 1402008 mar seo a leanas déanfaidh an coimisiún rialacha mionsonraithe maidir le cur chun feidhme airteagal 14 den chomhaontú eatramhach agus dá éis airteagal 29 de ccc maidir leis na taraifchuótaí diasc agus do tháirgí iascaigh a ghlacadh i gcomhréir leis an nós imeachta scrúdúcháin dá dtagraítear in airteagal 8a3 den rialachán seo\n"
          ]
        }
      ],
      "source": [
        "print(irish.longestSent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oauhQ1fjsC69"
      },
      "source": [
        "## Task 2. Model Implementation and Training (30 pts)\n",
        "\n",
        "\n",
        "\n",
        "## Task 2a. Encoder-Decoder Model Implementation (10 pts)\n",
        "Implement an Encoder-Decoder model in Pytorch with the following components\n",
        "* A single layer RNN based encoder. \n",
        "* A single layer RNN based decoder\n",
        "* A Encoder-Decoder model based on the above components that support sequence-to-sequence modelling. For the encoder/decoder you can use RNN, LSTMs or GRU. Use a hidden dimension of 256 or less depending on your compute constraints. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes of train source (10000, 10), and target (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "# Your Code Here\n",
        "#Ref: Dr Paul Buitelaar/Dr Omnia Zayed - Lab 08 \"Neural NMT\"\n",
        "\n",
        "'''\n",
        "This function is used to take sentences\n",
        "from both the dataset and converting it\n",
        "into embeddings to load it into the torch\n",
        "model and padding sequences into a list,\n",
        "and then converting source and target\n",
        "embeddings to train, development and test\n",
        "sets.\n",
        "'''\n",
        "def encode_features(\n",
        "    df: pd.DataFrame, \n",
        "    english: Lang,\n",
        "    irish: Lang,\n",
        "    pad_token: int = 0,\n",
        "    max_seq_length = 10\n",
        "  ):\n",
        "\n",
        "  source = []\n",
        "  target = []\n",
        "\n",
        "  for _, row in df.iterrows():\n",
        "    source.append(english.encodeSent(row[\"EngText\"]))\n",
        "    target.append(irish.encodeSent(row[\"IrText\"]))\n",
        "\n",
        "  source = pad_sequences(\n",
        "      source,\n",
        "      maxlen=max_seq_length,\n",
        "      padding=\"post\",\n",
        "      truncating = \"post\",\n",
        "      value=pad_token\n",
        "    )\n",
        "  \n",
        "  target = pad_sequences(\n",
        "      target,\n",
        "      maxlen=max_seq_length,\n",
        "      padding=\"post\",\n",
        "      truncating = \"post\",\n",
        "      value=pad_token\n",
        "    )\n",
        "  \n",
        "  return source, target\n",
        "\n",
        "train_source, train_target = encode_features(df_train, english, irish)\n",
        "dev_source, dev_target = encode_features(df_dev, english, irish)\n",
        "test_source, test_target = encode_features(df_test1, english, irish)\n",
        "\n",
        "print(f\"Shapes of train source {train_source.shape}, and target {train_target.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ref: Dr Paul Buitelaar/Dr Omnia Zayed - Lab 08 \"Neural NMT\"\n",
        "'''\n",
        "Converting embeddings done above into\n",
        "tensor data so that it can be fed to \n",
        "training the model, with keeping batch\n",
        "size as 32 for each set.\n",
        "'''\n",
        "train_dl = DataLoader(\n",
        "    TensorDataset(\n",
        "        torch.LongTensor(train_source),\n",
        "        torch.LongTensor(train_target)\n",
        "    ),\n",
        "    shuffle = True,\n",
        "    batch_size = 32\n",
        ")\n",
        "\n",
        "dev_dl = DataLoader(\n",
        "    TensorDataset(\n",
        "        torch.LongTensor(dev_source),\n",
        "        torch.LongTensor(dev_target)\n",
        "    ),\n",
        "    shuffle = False,\n",
        "    batch_size = 32\n",
        ")\n",
        "\n",
        "test_dl = DataLoader(\n",
        "    TensorDataset(\n",
        "        torch.LongTensor(test_source),\n",
        "        torch.LongTensor(test_target)\n",
        "    ),\n",
        "    shuffle = False,\n",
        "    batch_size = 32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ref: https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
        "'''\n",
        "Creating an encoder where RNN is used to convert\n",
        "embeddings and creating an encoding output, also\n",
        "using dropout to drop 50% data randomly.\n",
        "'''\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_vocab_size, \n",
        "                 encoder_hid_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 dropout_prob = .5):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_vocab_size, encoder_hid_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(encoder_hid_dim, hid_dim, n_layers, dropout = dropout_prob)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ref: https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
        "'''\n",
        "Creating an decoder where LSTM is used to convert\n",
        "encoder output to find IDs for each embeddings so\n",
        "that it converts the IDs to real words, and also\n",
        "using dropout to drop 50% data randomly.\n",
        "'''\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 target_vocab_size, \n",
        "                 dec_hid_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 dropout_prob = .5):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = target_vocab_size\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(target_vocab_size, dec_hid_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(dec_hid_dim, hid_dim, n_layers, dropout = dropout_prob)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, target_vocab_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ref: https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
        "'''\n",
        "Merging Encoder and Decoder into new model where\n",
        "forward pass to grab encoder outputs and decoding\n",
        "the values found while translating using decoder.\n",
        "'''\n",
        "class EncoderDecoderLSTM(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "        \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EncoderDecoderLSTM(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(9039, 256)\n",
              "    (rnn): LSTM(256, 128, num_layers=128, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(12173, 256)\n",
              "    (rnn): LSTM(256, 128, num_layers=128, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=128, out_features=12173, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT_DIM = english.n_words\n",
        "OUTPUT_DIM = irish.n_words\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 128\n",
        "DEC_HID_DIM = 128\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "model_lstm = EncoderDecoderLSTM(enc, dec)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model_lstm.apply(init_weights)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2b. Training (10 pts)\n",
        "Implement the code to train the Encoder-Decoder model on the Irish-English data. You will write code for the following:\n",
        "* Training, validation and test dataloaders \n",
        "* A training loop which trains the model for 5 epoch. Evaluate the loop at the end of each Epoch. Print out the train perplexity and validation perplexity after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model_lstm.parameters())\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_lstm.to(device)\n",
        "\n",
        "EPOCHS = 5\n",
        "best_val_loss = float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9b6e2069bdc4dcfb60a5856080f6795",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9631038547f4ec39b83f4eee5e881b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | train loss 5.748 | train ppl 313.56290692773194 | val ppl 150.05471586255422\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9dfdca655104a029a3b7e173056d95d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b2199d160ce4e76a375ceffcfdcef57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | train loss 5.144 | train ppl 171.39999894354537 | val ppl 139.770249560003\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6017521b2f14923aa8d3176bfba1e23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33a7fb34e72344e195c3a543ddc098eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | train loss 5.051 | train ppl 156.1785649881239 | val ppl 138.3795123399606\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fc72138f8d4443d81f517cc467f208b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e511f2fe0c34730968b34eb632335f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | train loss 4.999 | train ppl 148.26482012532418 | val ppl 138.2412019943194\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "308a9805bbba4b29817d6e0d91ee536e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60cb3513964448e0a0047f52fe57c70f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | train loss 4.963 | train ppl 143.02221959891523 | val ppl 139.49098841511483\n"
          ]
        }
      ],
      "source": [
        "#Ref: https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  model_lstm.train()\n",
        "  epoch_loss = 0\n",
        "  for batch in tqdm(train_dl, total=len(train_dl)):\n",
        "\n",
        "     src = batch[0].transpose(1, 0).to(device)\n",
        "     trg = batch[1].transpose(1, 0).to(device)\n",
        "\n",
        "     optimizer.zero_grad()\n",
        "\n",
        "     output = model_lstm(src, trg)\n",
        "\n",
        "     output_dim = output.shape[-1]\n",
        "     output = output[1:].view(-1, output_dim).to(device)\n",
        "     trg = trg[1:].reshape(-1)\n",
        "     \n",
        "     loss = F.cross_entropy(output, trg)\n",
        "     loss.backward()\n",
        "\n",
        "     torch.nn.utils.clip_grad_norm_(model_lstm.parameters(), 1)\n",
        "     optimizer.step()\n",
        "     epoch_loss += loss.item()\n",
        "\n",
        "  train_loss = round(epoch_loss / len(train_dl), 3)\n",
        "  \n",
        "  eval_loss = 0\n",
        "  model_lstm.eval()\n",
        "  for batch in tqdm(dev_dl, total=len(dev_dl)):\n",
        "    src = batch[0].transpose(1, 0).to(device)\n",
        "    trg = batch[1].transpose(1, 0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output = model_lstm(src, trg)\n",
        "      \n",
        "      output_dim = output.shape[-1]\n",
        "      output = output[1:].view(-1, output_dim).to(device)\n",
        "      trg = trg[1:].reshape(-1)\n",
        "      \n",
        "      loss = F.cross_entropy(output, trg)\n",
        "      \n",
        "      eval_loss += loss.item()\n",
        "  \n",
        "  dev_loss = round(eval_loss / len(dev_dl), 3)\n",
        "  print(f\"Epoch {epoch} | train loss {train_loss} | train ppl {np.exp(train_loss)} | val ppl {np.exp(dev_loss)}\")\n",
        "\n",
        "\n",
        "  if dev_loss < best_val_loss:\n",
        "    best_val_loss = dev_loss\n",
        "    torch.save(model_lstm.state_dict(), 'best-model-lstm.pt')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_lstm.load_state_dict(torch.load(\"best-model-lstm.pt\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2c. Evaluation on the Test Set (10 pts)\n",
        "Use the trained model to translate the text from the source language into the target language on the test set. Evaluate the performance of the model on the test set using the BLEU metric and print out the average the BLEU score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "This function takes the english sentence \n",
        "and returns a translated output back to us \n",
        "which was done via the neural machine \n",
        "translator\n",
        "'''\n",
        "\n",
        "def translate_sentence(\n",
        "    text: str, \n",
        "    model: EncoderDecoderLSTM, \n",
        "    english: Lang,\n",
        "    irish: Lang,\n",
        "    device: str,\n",
        "    max_len: int = 10,\n",
        "  ) -> str:\n",
        "\n",
        "  # Encode english sentence and convert to tensor\n",
        "  input_ids = english.encodeSent(text)\n",
        "  input_tensor = torch.LongTensor(input_ids).unsqueeze(1).to(device)\n",
        "\n",
        "  # Get encooder hidden states\n",
        "  with torch.no_grad():\n",
        "    encoder_outputs, hidden = model.encoder(input_tensor)\n",
        "\n",
        "  # Build target holder list\n",
        "  trg_indexes = [irish.word2index[\"<bof>\"]]\n",
        "\n",
        "  # Loop over sequence length of target sentence\n",
        "  for i in range(max_len):\n",
        "    trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "    \n",
        "    # Decode the encoder outputs with respect to current target word\n",
        "    with torch.no_grad():\n",
        "      output, hidden, cell = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
        "    \n",
        "    # Retrieve most likely word over target distribution\n",
        "    pred_token = torch.argmax(output).item()\n",
        "    trg_indexes.append(pred_token)\n",
        "\n",
        "    if pred_token == irish.word2index[\"<eof>\"]:\n",
        "      break\n",
        "\n",
        "  return \"\".join(irish.decodeIds(trg_indexes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "This three functions work to translate\n",
        "english sentences each row, does the \n",
        "preprocessing on the passed sentence\n",
        "and returns the output to the bleu score\n",
        "function which then calculates bleu score\n",
        "for each row in the test dataframe.\n",
        "'''\n",
        "\n",
        "def auto_translate_lstm(sentence: str):\n",
        "    output =  translate_sentence(sentence, model_lstm, english, irish, device)\n",
        "    return output\n",
        "\n",
        "def blue_score(reference_sentence, condidate_sentence):\n",
        "  return len([word for word in condidate_sentence if word in reference_sentence])/len(reference_sentence)\n",
        "\n",
        "def test_preprocessing(data: str):\n",
        "        data = data.lower()\n",
        "        data = re.sub(r'[^\\w\\s]', '', data).strip()\n",
        "        data = data.replace(\"\\n\", \" \")\n",
        "        data = \"<bof>\" + data + \"<eof>\"\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09bef4489e7146aca756edebd6037aa3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for index, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
        "    df_test.loc[index, \"TranslatedText\"] = auto_translate_lstm(row[\"EngText\"])\n",
        "    df_test.loc[index, \"BLEU Score\"] = blue_score(test_preprocessing(row[\"IrText\"]), auto_translate_lstm(row[\"EngText\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EngText</th>\n",
              "      <th>IrText</th>\n",
              "      <th>TranslatedText</th>\n",
              "      <th>BLEU Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Procès-verbal of rectification to the Conventi...</td>\n",
              "      <td>Miontuairisc cheartaitheach maidir le Coinbhin...</td>\n",
              "      <td>&lt;bof&gt; PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD</td>\n",
              "      <td>0.073529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(Official Journal of the European Union L 147 ...</td>\n",
              "      <td>(Iris Oifigiúil an Aontais Eorpaigh L 147 an 1...</td>\n",
              "      <td>&lt;bof&gt; PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD</td>\n",
              "      <td>0.111940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in Switzerland the higher cantonal court,’;</td>\n",
              "      <td>san Eilvéis ardchúirt an chantúin,”;</td>\n",
              "      <td>&lt;bof&gt; an PAD PAD PAD PAD PAD PAD PAD PAD PAD</td>\n",
              "      <td>0.395349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PREAMBLE</td>\n",
              "      <td>BROLLACH</td>\n",
              "      <td>&lt;bof&gt; PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD</td>\n",
              "      <td>0.277778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>However, in proceedings which have as their ob...</td>\n",
              "      <td>a bheith i scríbhinn nó arna fhianú i scríbhin...</td>\n",
              "      <td>&lt;bof&gt; PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             EngText  \\\n",
              "0  Procès-verbal of rectification to the Conventi...   \n",
              "1  (Official Journal of the European Union L 147 ...   \n",
              "2        in Switzerland the higher cantonal court,’;   \n",
              "3                                           PREAMBLE   \n",
              "4  However, in proceedings which have as their ob...   \n",
              "\n",
              "                                              IrText  \\\n",
              "0  Miontuairisc cheartaitheach maidir le Coinbhin...   \n",
              "1  (Iris Oifigiúil an Aontais Eorpaigh L 147 an 1...   \n",
              "2               san Eilvéis ardchúirt an chantúin,”;   \n",
              "3                                           BROLLACH   \n",
              "4  a bheith i scríbhinn nó arna fhianú i scríbhin...   \n",
              "\n",
              "                                  TranslatedText  BLEU Score  \n",
              "0  <bof> PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD    0.073529  \n",
              "1  <bof> PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD    0.111940  \n",
              "2   <bof> an PAD PAD PAD PAD PAD PAD PAD PAD PAD    0.395349  \n",
              "3  <bof> PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD    0.277778  \n",
              "4  <bof> PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD    0.250000  "
            ]
          },
          "execution_count": 175,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score of `LSTM` Model: 0.3142213818144513\n"
          ]
        }
      ],
      "source": [
        "mean = df_test[\"BLEU Score\"].mean()\n",
        "print(f\"Average BLEU score of `LSTM` Model: {mean}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3. Improving NMT using Attention (10 pts) \n",
        "Extend the Encoder-Decoder model from Task 2 with the attention mechanism. Retrain the model and evaluate on test set. Print the updated average BLEU score on the test set. In a few sentences explains which model is the best for translation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ref: Dr Paul Buitelaar/Dr Omnia Zayed - Lab 08 \"Neural NMT\"\n",
        "\n",
        "class EncoderGRU(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        input_vocab_size,  # size of source vocabulary  \n",
        "        hidden_dim,        # hidden dimension of embeddings\n",
        "        encoder_hid_dim,   # gru hidden dim\n",
        "        decoder_hid_dim,   # decoder hidden dim \n",
        "        dropout_prob = .5\n",
        "      ):\n",
        "      \n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_vocab_size, hidden_dim)\n",
        "        self.rnn = nn.GRU(hidden_dim, encoder_hid_dim, bidirectional = True)\n",
        "        self.fc = nn.Linear(encoder_hid_dim * 2, decoder_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards GRU\n",
        "        #hidden [-1, :, : ] is the last of the backwards GRU\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))        \n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ref: Dr Paul Buitelaar/Dr Omnia Zayed - Lab 08 \"Neural NMT\"\n",
        "'''\n",
        "The attention layer is responsible to\n",
        "generate a number for each word in the\n",
        "training data so that the importance of\n",
        "each word can be generated and used to \n",
        "output the next word in the seequence,\n",
        "and also makes the encoder-decoder model\n",
        "work faster than the previous.\n",
        "'''\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        enc_hid_dim,      # Encoder hidden dimension\n",
        "        dec_hid_dim       # Decoder hidden dimension \n",
        "      ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        #attention output: [batch size, src len]\n",
        "        return F.softmax(attention, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ref: Dr Paul Buitelaar/Dr Omnia Zayed - Lab 08 \"Neural NMT\"\n",
        "\n",
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        target_vocab_size,    # Size of target vocab \n",
        "        hidden_dim,           # hidden size of embedding  \n",
        "        enc_hid_dim, \n",
        "        dec_hid_dim, \n",
        "        dropout\n",
        "      ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = target_vocab_size\n",
        "        self.attention = Attention(enc_hid_dim, dec_hid_dim)\n",
        "        \n",
        "        self.embedding = nn.Embedding(target_vocab_size, hidden_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + hidden_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(\n",
        "            (enc_hid_dim * 2) + dec_hid_dim + hidden_dim, \n",
        "            target_vocab_size\n",
        "          )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(0)  # [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))  # [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)     # [batch size, src len]\n",
        "        a = a.unsqueeze(1)                              # [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2) # [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)           # [batch size, 1, enc hid dim * 2]\n",
        "        weighted = weighted.permute(1, 0, 2)               # [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2) # [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]    \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1)) # [batch size, output dim]\n",
        "        return prediction, hidden.squeeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ref: Dr Paul Buitelaar/Dr Omnia Zayed - Lab 08 \"Neural NMT\"\n",
        "\n",
        "class EncoderDecoderAtt(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time     \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):     \n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EncoderDecoderAtt(\n",
              "  (encoder): EncoderGRU(\n",
              "    (embedding): Embedding(9039, 256)\n",
              "    (rnn): GRU(256, 128, bidirectional=True)\n",
              "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): DecoderGRU(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=384, out_features=128, bias=True)\n",
              "      (v): Linear(in_features=128, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(12173, 256)\n",
              "    (rnn): GRU(512, 128)\n",
              "    (fc_out): Linear(in_features=640, out_features=12173, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT_DIM = english.n_words\n",
        "OUTPUT_DIM = irish.n_words\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 128\n",
        "DEC_HID_DIM = 128\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = EncoderGRU(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = DecoderGRU(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "model_att = EncoderDecoderAtt(enc, dec)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model_att.apply(init_weights)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yqdYhxa1uiqF"
      },
      "source": [
        "## Task 3a. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your Code Here\n",
        "\n",
        "optimizer = torch.optim.Adam(model_att.parameters())\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_att.to(device)\n",
        "\n",
        "EPOCHS = 5\n",
        "best_val_loss = float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "4cZ-6zHtwkZn"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b9d6b81a4c148ba8fe6678c5a303022",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8494f7fbeb504d01abdec337ce1d7f2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | train loss 5.44 | train ppl 230.44218346064218 | val ppl 104.37602463655413\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d596683276245dbb2982195225a8e38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85c72e1cf07f45babda3ae09a964c92a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | train loss 4.672 | train ppl 106.91135145513537 | val ppl 90.64946179433973\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec45ae67f8744e3ea3a1dcd67dbe8eac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb8e13b3e84d49e08d733626b114d34b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | train loss 4.463 | train ppl 86.74736120689518 | val ppl 75.1134772455169\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5adea5eeda6d433595b9c687e9872bdb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37e8b9389592484084e9743b4fe4c76b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | train loss 4.322 | train ppl 75.33915602616537 | val ppl 73.4055833378014\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2512eb112ec427f80609bdb7a9bd948",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3387085c864449099b72ea730cd107f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | train loss 4.151 | train ppl 63.49746602599656 | val ppl 71.37873529593749\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "As we have saved the weights below, we wont\n",
        "train the model again and so commenting out\n",
        "the code and using load_state.dict(torch.load())\n",
        "below on the model class to call back the best\n",
        "weights saved to save time\n",
        "'''\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  model_att.train()\n",
        "  epoch_loss = 0\n",
        "  for batch in tqdm(train_dl, total=len(train_dl)):\n",
        "\n",
        "     src = batch[0].transpose(1, 0).to(device)\n",
        "     trg = batch[1].transpose(1, 0).to(device)\n",
        "\n",
        "     optimizer.zero_grad()\n",
        "\n",
        "     output = model_att(src, trg)\n",
        "\n",
        "     output_dim = output.shape[-1]\n",
        "     output = output[1:].view(-1, output_dim).to(device)\n",
        "     trg = trg[1:].reshape(-1)\n",
        "     \n",
        "     loss = F.cross_entropy(output, trg)\n",
        "     loss.backward()\n",
        "\n",
        "     torch.nn.utils.clip_grad_norm_(model_att.parameters(), 1)\n",
        "     optimizer.step()\n",
        "     epoch_loss += loss.item()\n",
        "\n",
        "  train_loss = round(epoch_loss / len(train_dl), 3)\n",
        "  \n",
        "  eval_loss = 0\n",
        "  model_att.eval()\n",
        "  for batch in tqdm(dev_dl, total=len(dev_dl)):\n",
        "    src = batch[0].transpose(1, 0).to(device)\n",
        "    trg = batch[1].transpose(1, 0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output = model_att(src, trg)\n",
        "      \n",
        "      output_dim = output.shape[-1]\n",
        "      output = output[1:].view(-1, output_dim).to(device)\n",
        "      trg = trg[1:].reshape(-1)\n",
        "      \n",
        "      loss = F.cross_entropy(output, trg)\n",
        "      \n",
        "      eval_loss += loss.item()\n",
        "  \n",
        "  dev_loss = round(eval_loss / len(dev_dl), 3)\n",
        "  print(f\"Epoch {epoch} | train loss {train_loss} | train ppl {np.exp(train_loss)} | val ppl {np.exp(dev_loss)}\")\n",
        "\n",
        "\n",
        "  if dev_loss < best_val_loss:\n",
        "    best_val_loss = dev_loss\n",
        "    torch.save(model_att.state_dict(), 'best-model-att.pt')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 184,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_att.load_state_dict(torch.load(\"best-model-att.pt\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QofrQ1GAwnDz"
      },
      "source": [
        "# Task 3b. Evaluation on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "OJP145YuxAgq"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "'''\n",
        "This function takes the english sentence \n",
        "and returns a translated output back to us \n",
        "which was done via the neural machine \n",
        "translator\n",
        "'''\n",
        "\n",
        "def translate_sentence(\n",
        "    text: str, \n",
        "    model: EncoderDecoderAtt, \n",
        "    english: Lang,\n",
        "    irish: Lang,\n",
        "    device: str,\n",
        "    max_len: int = 10,\n",
        "  ) -> str:\n",
        "\n",
        "  # Encode english sentence and convert to tensor\n",
        "  input_ids = english.encodeSent(text)\n",
        "  input_tensor = torch.LongTensor(input_ids).unsqueeze(1).to(device)\n",
        "\n",
        "  # Get encooder hidden states\n",
        "  with torch.no_grad():\n",
        "    encoder_outputs, hidden = model.encoder(input_tensor)\n",
        "\n",
        "  # Build target holder list\n",
        "  trg_indexes = [irish.word2index[\"<bof>\"]]\n",
        "\n",
        "  # Loop over sequence length of target sentence\n",
        "  for i in range(max_len):\n",
        "    trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "    \n",
        "    # Decode the encoder outputs with respect to current target word\n",
        "    with torch.no_grad():\n",
        "      output, hidden = model.decoder(trg_tensor, hidden, encoder_outputs)\n",
        "    \n",
        "    # Retrieve most likely word over target distribution\n",
        "    pred_token = torch.argmax(output).item()\n",
        "    trg_indexes.append(pred_token)\n",
        "\n",
        "    if pred_token == irish.word2index[\"<eof>\"]:\n",
        "      break\n",
        "\n",
        "  return \"\".join(irish.decodeIds(trg_indexes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "This three functions work to translate\n",
        "english sentences each row, does the \n",
        "preprocessing (above) on the passed sentence\n",
        "and returns the output to the bleu score\n",
        "function (above) which then calculates bleu score\n",
        "for each row in the test dataframe.\n",
        "'''\n",
        "def auto_translate_att(sentence: str):\n",
        "    output =  translate_sentence(sentence, model_att, english, irish, device)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84795cca86e14778824d27962baca48b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for index, row in tqdm(df_test1.iterrows(), total=len(df_test1)):\n",
        "    df_test1.loc[index, \"TranslatedText\"] = auto_translate_att(row[\"EngText\"])\n",
        "    df_test1.loc[index, \"BLEU Score\"] = blue_score(test_preprocessing(row[\"IrText\"]), auto_translate_att(row[\"EngText\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EngText</th>\n",
              "      <th>IrText</th>\n",
              "      <th>TranslatedText</th>\n",
              "      <th>BLEU Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Procès-verbal of rectification to the Conventi...</td>\n",
              "      <td>Miontuairisc cheartaitheach maidir le Coinbhin...</td>\n",
              "      <td>&lt;bof&gt; airteagal &lt;eof&gt;</td>\n",
              "      <td>0.102941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(Official Journal of the European Union L 147 ...</td>\n",
              "      <td>(Iris Oifigiúil an Aontais Eorpaigh L 147 an 1...</td>\n",
              "      <td>&lt;bof&gt; airteagal &lt;eof&gt;</td>\n",
              "      <td>0.156716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in Switzerland the higher cantonal court,’;</td>\n",
              "      <td>san Eilvéis ardchúirt an chantúin,”;</td>\n",
              "      <td>&lt;bof&gt; airteagal &lt;eof&gt;</td>\n",
              "      <td>0.465116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PREAMBLE</td>\n",
              "      <td>BROLLACH</td>\n",
              "      <td>&lt;bof&gt; ciallaíonn an &lt;eof&gt;</td>\n",
              "      <td>0.944444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>However, in proceedings which have as their ob...</td>\n",
              "      <td>a bheith i scríbhinn nó arna fhianú i scríbhin...</td>\n",
              "      <td>&lt;bof&gt; airteagal &lt;eof&gt;</td>\n",
              "      <td>0.316667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             EngText  \\\n",
              "0  Procès-verbal of rectification to the Conventi...   \n",
              "1  (Official Journal of the European Union L 147 ...   \n",
              "2        in Switzerland the higher cantonal court,’;   \n",
              "3                                           PREAMBLE   \n",
              "4  However, in proceedings which have as their ob...   \n",
              "\n",
              "                                              IrText  \\\n",
              "0  Miontuairisc cheartaitheach maidir le Coinbhin...   \n",
              "1  (Iris Oifigiúil an Aontais Eorpaigh L 147 an 1...   \n",
              "2               san Eilvéis ardchúirt an chantúin,”;   \n",
              "3                                           BROLLACH   \n",
              "4  a bheith i scríbhinn nó arna fhianú i scríbhin...   \n",
              "\n",
              "              TranslatedText  BLEU Score  \n",
              "0      <bof> airteagal <eof>    0.102941  \n",
              "1      <bof> airteagal <eof>    0.156716  \n",
              "2      <bof> airteagal <eof>    0.465116  \n",
              "3  <bof> ciallaíonn an <eof>    0.944444  \n",
              "4      <bof> airteagal <eof>    0.316667  "
            ]
          },
          "execution_count": 188,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score of `Attention` Model: 0.4730971935081272\n"
          ]
        }
      ],
      "source": [
        "mean = df_test1[\"BLEU Score\"].mean()\n",
        "print(f\"Average BLEU score of `Attention` Model: {mean}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
